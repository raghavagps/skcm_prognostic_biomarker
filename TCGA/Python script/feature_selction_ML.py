# -*- coding: utf-8 -*-
"""SKCM_svc-l1_20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pThu3Et_6IK2zOR68qxZNIM18zxFFjUO
"""

import pandas as pd
import numpy as np
df = pd.read_csv('/content/OS_time_in_days.csv')
df

# Remove the first column using iloc
df = df.iloc[:, 1:]

print("DataFrame after removing the first column using iloc:")
print(df)

# Sort the DataFrame by the os_time column in ascending order
df_sorted = df.sort_values(by='OS_in_days', ascending=True).reset_index(drop=True)

# Display the sorted DataFrame
print(df_sorted)

# Step 2: Define a function to map OS_in_days to classes
def map_os_class(OS_in_days):
    years = OS_in_days / 365.25  # Convert days to years
    if years <= 1:
        return 0  # 0-1 year
    elif years <= 3:
        return 1  # 1-3 years
    elif years <= 5:
        return 2  # 3-5 years
    else:
        return 3  # more than 5 years

# Step 3: Create OS class column for training and independent DataFrames
df_sorted['os_class'] = df_sorted['OS_in_days'].apply(map_os_class)

from imblearn.over_sampling import SMOTE
# Separate the target variable and the rest of the data
X = df_sorted.drop(['os_class','OS_in_days'], axis=1)
y = df_sorted['os_class']
# Apply SMOTE to oversample the minority class
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)
# Print the class distribution before and after SMOTE
print("Class distribution before SMOTE:")
print(y.value_counts())
print("Class distribution after SMOTE:")
print(y_resampled.value_counts())
# Update X and y with the resampled data
X = X_resampled
y = y_resampled
# Create a new DataFrame with the resampled data
SMOTEDdf = pd.concat([X, y], axis=1)
SMOTEDdf

# Step 1: Split the dataset into training and testing
training_sets = []
independent_sets = []

for i in range(0, len(SMOTEDdf), 5):
    independent_set = SMOTEDdf.iloc[i:i + 1]  # Select the i-th row for testing
    training_set = SMOTEDdf.iloc[i + 1:i + 5]  # Select the next 4 rows for training

    if not independent_set.empty:
        independent_sets.append(independent_set)

    if not training_set.empty:
        training_sets.append(training_set)

# Combine the training and independent sets into DataFrames
independent_df = pd.concat(independent_sets, ignore_index=True)
training_df = pd.concat(training_sets, ignore_index=True)

independent_df

training_df

# Save DataFrame to CSV
training_df.to_csv('training_df.csv', index=False)

# Save DataFrame to CSV
independent_df.to_csv('independent_df.csv', index=False)

# Step 2: Reshuffle both the training and independent datasets
training_df = training_df.sample(frac=1, random_state=42).reset_index(drop=True)
independent_df = independent_df.sample(frac=1, random_state=42).reset_index(drop=True)

## Based on feature importance top 20
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd

# Step 1: Separate features and target variable
X_train = training_df.drop('os_class', axis=1)
y_train = training_df['os_class']

# Step 2: Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

X_test = independent_df.drop('os_class', axis=1)
y_test = independent_df['os_class']
X_test_scaled = scaler.transform(X_test)

# Step 3: Initialize and fit LinearSVC with L1 penalty
svc_l1 = LinearSVC(penalty='l1', dual=False, C=1, random_state=42, max_iter=10000)
svc_l1.fit(X_train_scaled, y_train)

# Step 4: Create SelectFromModel and select the top 20 features
model = SelectFromModel(svc_l1, threshold="mean", max_features=20, prefit=True)

# Step 5: Transform both the training and test data to only keep the selected features
X_train_selected = model.transform(X_train_scaled)
X_test_selected = model.transform(X_test_scaled)

# Step 6: Check the shape of the selected data
print(f"X_train_selected shape: {X_train_selected.shape}")
print(f"X_test_selected shape: {X_test_selected.shape}")

# Verify that exactly 20 features are selected
print(f"Number of selected features: {X_train_selected.shape[1]}")

# Step 7: Get the coefficients and their absolute values (importance)
coef_abs = np.abs(svc_l1.coef_).flatten()

# Step 8: Get the indices of the selected features
selected_indices = model.get_support(indices=True)

# Step 9: Get feature names of the selected features
selected_feature_names = X_train.columns[selected_indices]

# Step 10: Get the importance for selected features only
selected_importance = coef_abs[selected_indices]

# Step 11: Create a DataFrame for selected features and their importance
feature_importance = pd.DataFrame({
    'Feature': selected_feature_names,
    'Importance': selected_importance
})

# Step 12: Sort features by importance and rank them
top_20_features = feature_importance.sort_values(by='Importance', ascending=False)
top_20_features['Rank'] = range(1, len(top_20_features) + 1)  # Rank starts from 1

# Step 13: Display the top 20 features with rank and importance
print("Top 20 selected features with Rank and Importance:")
print(top_20_features)

# Step 14: Get the top 20 selected features as an array for further use
top_20_feature_names = top_20_features['Feature'].values
print("Top 20 selected feature names (as array):")
print(top_20_feature_names)



# Check the dimensions of X_train_selected
print("Training data shape:", X_train_selected.shape)

# Check the dimensions of X_test_selected
print("Test data shape:", X_test_selected.shape)

train_selected = pd.DataFrame(X_train_selected, columns=top_20_feature_names)
# Save the train_data to a CSV file
train_selected.to_csv('X_train_data_20_imp.csv', index=False)  # Set index=False to avoid saving the index column

test_selected = pd.DataFrame(X_test_selected, columns=top_20_feature_names)
# Save the test_data to a CSV file
test_selected.to_csv('X_test_data_20_imp.csv', index=False)

train_selected

test_selected

y_train.to_csv('y_train_20.csv', index=False)
y_test.to_csv('y_test_20.csv', index=False)

y_train

y_test

from google.colab import files## optional

# Download the CSV file
files.download('X_train_data_20_imp.csv')
files.download('X_test_data_20_imp.csv')
files.download('y_train_20.csv')
files.download('y_test_20.csv')



import pandas as pd

# Concatenate the selected features with the target labels (y_train and y_test)
train_data = pd.concat([pd.DataFrame(train_selected), y_train.reset_index(drop=True)], axis=1)
test_data = pd.concat([pd.DataFrame(test_selected), y_test.reset_index(drop=True)], axis=1)

# Print the first few rows to verify
print("Training data:")
print(train_data.head())

print("Testing data:")
print(test_data.head())
train_data.to_csv('train_20_svc_imp.csv', index=False)  # Set index=False to avoid saving the index column
test_data.to_csv('test_20_svc_imp.csv', index=False)



from google.colab import files## optional

# Download the CSV file
files.download('train_20_svc_imp.csv')
files.download('test_20_svc_imp.csv')

import pandas as pd
import numpy as np
df = pd.read_csv('/content/X_train_data_20_imp.csv')

df

df1 = pd.read_csv('/content/X_test_data_20_imp.csv')

y_train = pd.read_csv('/content/y_train_20.csv')

y_train

y_test = pd.read_csv('/content/y_test_20.csv')

# Convert DataFrame to NumPy array
X_train_selected = df.values  # or df.to_numpy()

print("NumPy Array:")
print(X_train_selected)

# Convert DataFrame to NumPy array
X_test_selected = df1.values  # or df.to_numpy()

print("NumPy Array:")
print(X_test_selected)

X_train_selected

X_test_selected

# Step 6: Get the names of the selected features
#selected_features = X_train.columns[model.get_support()]

# Count the number of selected features
#num_selected_features = len(selected_features)
#print("Selected features:", selected_features)
#print("Number of selected features:", num_selected_features)

"""Performace on top 20 selected by svc-l1 on the basis of feature importance"""

##
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.svm import SVC
#from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier  # Import XGBoost
from sklearn.metrics import (accuracy_score, confusion_matrix, roc_auc_score,
                             classification_report, cohen_kappa_score,
                             matthews_corrcoef, precision_score, recall_score)
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
import lightgbm as lgb
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
# Assume df_sorted has been prepared, class created, SMOTE applied, features selected, and scaled

# Step 1: Initialize classifiers (KNN, Decision Tree, and Naive Bayes removed)
#classifiers = {
   #'Random Forest': RandomForestClassifier(random_state=42,n_estimators=1000,bootstrap=False ,max_depth=15,min_samples_split=5,min_samples_leaf=2),
   #'SVM': SVC(probability=True, random_state=42, C=10, kernel='rbf', gamma='auto'),
    #'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42,penalty='l2', solver='liblinear', C=10),
  #'Extra Trees': ExtraTreesClassifier(random_state=42, n_estimators = 500, bootstrap=True, min_samples_split=3),  # Extra Trees Classifier
   #'XGBoost': XGBClassifier(random_state=42,n_estimators=500, max_depth=10, min_child_weight=5, reg_alpha=1, reg_lambda=1)

#}
# Adding additional classifiers
classifiers = {
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=1000, max_depth=11, min_samples_split=6, min_samples_leaf=3),
    'SVM': SVC(probability=True, random_state=42, C=10, kernel='rbf', gamma='auto'),
    #'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, penalty='l2', solver='liblinear', C=10),
    'Extra Trees': ExtraTreesClassifier(random_state=42, n_estimators=500, bootstrap=True, min_samples_split=3),
    'XGBoost': XGBClassifier(random_state=42, n_estimators=500, max_depth=10, min_child_weight=5, reg_alpha=1, reg_lambda=1),
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'LightGBM': lgb.LGBMClassifier(random_state=42, n_estimators=500, max_depth=10, min_child_samples=20),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=500, max_depth=10),
    'Naive Bayes': GaussianNB(),
    'MLP': MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)
}

# Prepare for results
results = []
#X_train_scaled = pd.read_csv('/content/train_selected.csv')
#X_test_scaled = pd.read_csv('/content/test_selected.csv')
# Step 2: Stratified Cross-Validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Step 3: Loop over each classifier
for model_name, model in classifiers.items():
    acc_train_list = []
    auc_train_list = []
    sens_train_list = []
    spec_train_list = []
    mcc_train_list = []
    kappa_train_list = []

    # Cross-validation loop
    for train_index, val_index in cv.split(X_train_selected, y_train):
        X_cv_train, X_cv_val = X_train_selected[train_index], X_train_selected[val_index]
        y_cv_train, y_cv_val = y_train.iloc[train_index], y_train.iloc[val_index]

        # Train the model
        model.fit(X_cv_train, y_cv_train)

        # Predictions
        y_cv_val_pred = model.predict(X_cv_val)
        y_cv_val_prob = model.predict_proba(X_cv_val)

        # Metrics
        acc_train = accuracy_score(y_cv_val, y_cv_val_pred)
        auc_train = roc_auc_score(y_cv_val, y_cv_val_prob, multi_class='ovr')
        sens_train = recall_score(y_cv_val, y_cv_val_pred, average='weighted')
        spec_train = precision_score(y_cv_val, y_cv_val_pred, average='weighted')
        mcc_train = matthews_corrcoef(y_cv_val, y_cv_val_pred)
        kappa_train = cohen_kappa_score(y_cv_val, y_cv_val_pred)

        # Store metrics
        acc_train_list.append(acc_train)
        auc_train_list.append(auc_train)
        sens_train_list.append(sens_train)
        spec_train_list.append(spec_train)
        mcc_train_list.append(mcc_train)
        kappa_train_list.append(kappa_train)

    # Average metrics for training
    results.append({
        'Model': model_name,
        'Train Accuracy': np.mean(acc_train_list),
        'Train AUC': np.mean(auc_train_list),
        'Train Sensitivity': np.mean(sens_train_list),
        'Train Specificity': np.mean(spec_train_list),
        'Train MCC': np.mean(mcc_train_list),
        'Train Kappa': np.mean(kappa_train_list),
    })

    # Test the model on independent test set
    model.fit(X_train_selected, y_train)
    y_test_pred = model.predict(X_test_selected)
    y_test_prob = model.predict_proba(X_test_selected)

    # Test metrics
    acc_test = accuracy_score(y_test, y_test_pred)
    auc_test = roc_auc_score(y_test, y_test_prob, multi_class='ovr')
    sens_test = recall_score(y_test, y_test_pred, average='weighted')
    spec_test = precision_score(y_test, y_test_pred, average='weighted')
    mcc_test = matthews_corrcoef(y_test, y_test_pred)
    kappa_test = cohen_kappa_score(y_test, y_test_pred)

    # Add testing metrics to results
    results[-1].update({
        'Test Accuracy': acc_test,
        'Test AUC': auc_test,
        'Test Sensitivity': sens_test,
        'Test Specificity': spec_test,
        'Test MCC': mcc_test,
        'Test Kappa': kappa_test,
    })

# Step 4: Convert results to DataFrame and display
results_df = pd.DataFrame(results)
print(results_df)

# Optionally, print classification reports for test set
for result in results:
    print(f"\nClassification Report for {result['Model']} (Test):")
    print(classification_report(y_test, y_test_pred))

#from google.colab import files

# Save results_df to a CSV file
#results_df.to_csv('skcm_evaluation_results_50.csv', index=False)

# Download the CSV file
#files.download('skcm_evaluation_results_50.csv')

!pip install catboost

"""Add Adaboost and Catboost"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier  # Import XGBoost
from sklearn.metrics import (accuracy_score, confusion_matrix, roc_auc_score,
                             classification_report, cohen_kappa_score,
                             matthews_corrcoef, precision_score, recall_score)
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
import lightgbm as lgb
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from catboost import CatBoostClassifier  # Import CatBoost

# Define classifiers including AdaBoost and CatBoost
classifiers = {
  'Random Forest': RandomForestClassifier(random_state=42, n_estimators=1000, max_depth=11, min_samples_split=6, min_samples_leaf=3),
    'SVM': SVC(probability=True, random_state=42, C=10, kernel='rbf', gamma='auto'),
    'Extra Trees': ExtraTreesClassifier(random_state=42, n_estimators=500, bootstrap=True, min_samples_split=3),
    'XGBoost': XGBClassifier(random_state=42, n_estimators=500, max_depth=10, min_child_weight=5, reg_alpha=1, reg_lambda=1),
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'LightGBM': lgb.LGBMClassifier(random_state=42, n_estimators=500, max_depth=10, min_child_samples=20),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=500, max_depth=10),
    'Naive Bayes': GaussianNB(),
    'MLP': MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42),
    'AdaBoost': AdaBoostClassifier(random_state=42, n_estimators=500),
    'CatBoost': CatBoostClassifier(random_state=42, iterations=800, depth=12, learning_rate=0.2, cat_features=[], verbose=1)
    }


# Prepare for results
results = []
# Assuming X_train_selected, y_train, X_test_selected, and y_test are pre-processed and available

# Stratified Cross-Validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Loop over each classifier
for model_name, model in classifiers.items():
    acc_train_list = []
    auc_train_list = []
    sens_train_list = []
    spec_train_list = []
    mcc_train_list = []
    kappa_train_list = []

    # Cross-validation loop
    for train_index, val_index in cv.split(X_train_selected, y_train):
        X_cv_train, X_cv_val = X_train_selected[train_index], X_train_selected[val_index]
        y_cv_train, y_cv_val = y_train.iloc[train_index], y_train.iloc[val_index]

        # Train the model
        model.fit(X_cv_train, y_cv_train)

        # Predictions
        y_cv_val_pred = model.predict(X_cv_val)
        y_cv_val_prob = model.predict_proba(X_cv_val)

        # Metrics
        acc_train = accuracy_score(y_cv_val, y_cv_val_pred)
        auc_train = roc_auc_score(y_cv_val, y_cv_val_prob, multi_class='ovr')
        sens_train = recall_score(y_cv_val, y_cv_val_pred, average='weighted')
        spec_train = precision_score(y_cv_val, y_cv_val_pred, average='weighted')
        mcc_train = matthews_corrcoef(y_cv_val, y_cv_val_pred)
        kappa_train = cohen_kappa_score(y_cv_val, y_cv_val_pred)

        # Store metrics
        acc_train_list.append(acc_train)
        auc_train_list.append(auc_train)
        sens_train_list.append(sens_train)
        spec_train_list.append(spec_train)
        mcc_train_list.append(mcc_train)
        kappa_train_list.append(kappa_train)

    # Average metrics for training
    results.append({
        'Model': model_name,
        'Train Accuracy': np.mean(acc_train_list),
        'Train AUC': np.mean(auc_train_list),
        'Train Sensitivity': np.mean(sens_train_list),
        'Train Specificity': np.mean(spec_train_list),
        'Train MCC': np.mean(mcc_train_list),
        'Train Kappa': np.mean(kappa_train_list),
    })

    # Test the model on independent test set
    model.fit(X_train_selected, y_train)
    y_test_pred = model.predict(X_test_selected)
    y_test_prob = model.predict_proba(X_test_selected)

    # Test metrics
    acc_test = accuracy_score(y_test, y_test_pred)
    auc_test = roc_auc_score(y_test, y_test_prob, multi_class='ovr')
    sens_test = recall_score(y_test, y_test_pred, average='weighted')
    spec_test = precision_score(y_test, y_test_pred, average='weighted')
    mcc_test = matthews_corrcoef(y_test, y_test_pred)
    kappa_test = cohen_kappa_score(y_test, y_test_pred)

    # Add testing metrics to results
    results[-1].update({
        'Test Accuracy': acc_test,
        'Test AUC': auc_test,
        'Test Sensitivity': sens_test,
        'Test Specificity': spec_test,
        'Test MCC': mcc_test,
        'Test Kappa': kappa_test,
    })

# Convert results to DataFrame and display
results = pd.DataFrame(results)
print(results)

from google.colab import files

# Save results_df to a CSV file
results.to_csv('svc-l1_20_imp.csv', index=False)

# Download the CSV file
files.download('svc-l1_20_imp.csv')

import pandas as pd
import numpy as np
results_df1 = pd.read_csv('/content/svc-l1_20_imp.csv')
results_df1

# Convert results to DataFrame and display
results_df2 = pd.DataFrame(results_df1)  # Convert results to a Pandas DataFrame
print(results_df2)

"""Ensemble Methods 1. Voting Classifier







"""

## voting classifier
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, matthews_corrcoef, cohen_kappa_score

# Define classifiers (RF, ET)
classifiers = {
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=1000, max_depth=15, min_samples_split=5, min_samples_leaf=2),
    'Extra Trees': ExtraTreesClassifier(random_state=42, n_estimators=500, bootstrap=True, min_samples_split=3)

}

# Create a Voting Classifier with 'soft' voting
voting_classifier = VotingClassifier(estimators=[(name, model) for name, model in classifiers.items()], voting='soft')

# Cross-validation and model evaluation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

results = []

for train_index, val_index in cv.split(X_train_selected, y_train):
    X_cv_train, X_cv_val = X_train_selected[train_index], X_train_selected[val_index]
    y_cv_train, y_cv_val = y_train.iloc[train_index], y_train.iloc[val_index]

    # Train the Voting Classifier
    voting_classifier.fit(X_cv_train, y_cv_train)

    # Predictions
    y_cv_val_pred = voting_classifier.predict(X_cv_val)
    y_cv_val_prob = voting_classifier.predict_proba(X_cv_val)

    # Calculate metrics
    acc_train = accuracy_score(y_cv_val, y_cv_val_pred)
    auc_train = roc_auc_score(y_cv_val, y_cv_val_prob, multi_class='ovr')
    sens_train = recall_score(y_cv_val, y_cv_val_pred, average='weighted')
    spec_train = precision_score(y_cv_val, y_cv_val_pred, average='weighted')
    mcc_train = matthews_corrcoef(y_cv_val, y_cv_val_pred)
    kappa_train = cohen_kappa_score(y_cv_val, y_cv_val_pred)

    # Store results
    results.append({
        'Model': 'Voting Classifier',
        'Train Accuracy': acc_train,
        'Train AUC': auc_train,
        'Train Sensitivity': sens_train,
        'Train Specificity': spec_train,
        'Train MCC': mcc_train,
        'Train Kappa': kappa_train,
    })

# Convert results to DataFrame
results_df1 = pd.DataFrame(results)
print(results_df1)

# Test the model on independent test set
voting_classifier.fit(X_train_selected, y_train)
y_test_pred = voting_classifier.predict(X_test_selected)
y_test_prob = voting_classifier.predict_proba(X_test_selected)

# Test metrics
acc_test = accuracy_score(y_test, y_test_pred)
auc_test = roc_auc_score(y_test, y_test_prob, multi_class='ovr')
sens_test = recall_score(y_test, y_test_pred, average='weighted')
spec_test = precision_score(y_test, y_test_pred, average='weighted')
mcc_test = matthews_corrcoef(y_test, y_test_pred)
kappa_test = cohen_kappa_score(y_test, y_test_pred)

# Add testing metrics to results
results_df1['Test Accuracy'] = acc_test
results_df1['Test AUC'] = auc_test
results_df1['Test Sensitivity'] = sens_test
results_df1['Test Specificity'] = spec_test
results_df1['Test MCC'] = mcc_test
results_df1['Test Kappa'] = kappa_test

print("\nVoting Classifier Test Metrics:")
print(results_df)



from sklearn.ensemble import VotingClassifier

# Step 1: Initialize individual classifiers
classifiers = {
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=1000, max_depth=15, min_samples_split=5, min_samples_leaf=2),
    'SVM': SVC(probability=True, random_state=42, C=10, kernel='rbf', gamma='auto'),
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, penalty='l2', solver='liblinear', C=10),
    'Extra Trees': ExtraTreesClassifier(random_state=42, n_estimators=500, bootstrap=True, min_samples_split=3),
    'XGBoost': XGBClassifier(random_state=42, n_estimators=500, max_depth=10, min_child_weight=5, reg_alpha=1, reg_lambda=1),
}

# Step 2: Create a VotingClassifier
voting_classifier = VotingClassifier(estimators=[(name, model) for name, model in classifiers.items()], voting='soft')

# Step 3: Train the model using cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Results storage
results = []

for train_index, val_index in cv.split(X_train_selected, y_train):
    X_cv_train, X_cv_val = X_train_selected[train_index], X_train_selected[val_index]
    y_cv_train, y_cv_val = y_train.iloc[train_index], y_train.iloc[val_index]

    # Train the VotingClassifier
    voting_classifier.fit(X_cv_train, y_cv_train)

    # Predictions
    y_cv_val_pred = voting_classifier.predict(X_cv_val)
    y_cv_val_prob = voting_classifier.predict_proba(X_cv_val)

    # Metrics
    acc_train = accuracy_score(y_cv_val, y_cv_val_pred)
    auc_train = roc_auc_score(y_cv_val, y_cv_val_prob, multi_class='ovr')
    sens_train = recall_score(y_cv_val, y_cv_val_pred, average='weighted')
    spec_train = precision_score(y_cv_val, y_cv_val_pred, average='weighted')
    mcc_train = matthews_corrcoef(y_cv_val, y_cv_val_pred)
    kappa_train = cohen_kappa_score(y_cv_val, y_cv_val_pred)

    # Store metrics
    results.append({
        'Model': 'Voting Classifier',
        'Train Accuracy': acc_train,
        'Train AUC': auc_train,
        'Train Sensitivity': sens_train,
        'Train Specificity': spec_train,
        'Train MCC': mcc_train,
        'Train Kappa': kappa_train,
    })

# Convert results to DataFrame and display
results_df = pd.DataFrame(results)
print(results_df)

# Test the model on independent test set
voting_classifier.fit(X_train_selected, y_train)
y_test_pred = voting_classifier.predict(X_test_selected)
y_test_prob = voting_classifier.predict_proba(X_test_selected)

# Test metrics
acc_test = accuracy_score(y_test, y_test_pred)
auc_test = roc_auc_score(y_test, y_test_prob, multi_class='ovr')
sens_test = recall_score(y_test, y_test_pred, average='weighted')
spec_test = precision_score(y_test, y_test_pred, average='weighted')
mcc_test = matthews_corrcoef(y_test, y_test_pred)
kappa_test = cohen_kappa_score(y_test, y_test_pred)

# Add testing metrics to results
results_df['Test Accuracy'] = acc_test
results_df['Test AUC'] = auc_test
results_df['Test Sensitivity'] = sens_test
results_df['Test Specificity'] = spec_test
results_df['Test MCC'] = mcc_test
results_df['Test Kappa'] = kappa_test

print("\nVoting Classifier Test Metrics:")
print(results_df)

"""2. Stacking Classifier"""

## stacking classifier
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
# Define base classifiers (RF, ET, XGB)
base_classifiers = [
    ('rf', RandomForestClassifier(random_state=42, n_estimators=1000, max_depth=15, min_samples_split=5, min_samples_leaf=2)),
    ('et', ExtraTreesClassifier(random_state=42, n_estimators=800, bootstrap=True, min_samples_split=3)),

]

# We can still use a simple meta-classifier or leave it as an empty model
meta_classifier = LogisticRegression(max_iter=1000, random_state=42, penalty='l2', solver='liblinear', C=10)

# Create StackingClassifier
stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=meta_classifier)

# Cross-validation and evaluation
results = []

for train_index, val_index in cv.split(X_train_selected, y_train):
    X_cv_train, X_cv_val = X_train_selected[train_index], X_train_selected[val_index]
    y_cv_train, y_cv_val = y_train.iloc[train_index], y_train.iloc[val_index]

    # Train the Stacking Classifier
    stacking_classifier.fit(X_cv_train, y_cv_train)

    # Predictions
    y_cv_val_pred = stacking_classifier.predict(X_cv_val)
    y_cv_val_prob = stacking_classifier.predict_proba(X_cv_val)

    # Calculate metrics
    acc_train = accuracy_score(y_cv_val, y_cv_val_pred)
    auc_train = roc_auc_score(y_cv_val, y_cv_val_prob, multi_class='ovr')
    sens_train = recall_score(y_cv_val, y_cv_val_pred, average='weighted')
    spec_train = precision_score(y_cv_val, y_cv_val_pred, average='weighted')
    mcc_train = matthews_corrcoef(y_cv_val, y_cv_val_pred)
    kappa_train = cohen_kappa_score(y_cv_val, y_cv_val_pred)

    # Store results
    results.append({
        'Model': 'Stacking Classifier',
        'Train Accuracy': acc_train,
        'Train AUC': auc_train,
        'Train Sensitivity': sens_train,
        'Train Specificity': spec_train,
        'Train MCC': mcc_train,
        'Train Kappa': kappa_train,
    })

# Convert results to DataFrame
results_df = pd.DataFrame(results)
print(results_df)

# Test the model on independent test set
stacking_classifier.fit(X_train_selected, y_train)
y_test_pred = stacking_classifier.predict(X_test_selected)
y_test_prob = stacking_classifier.predict_proba(X_test_selected)

# Test metrics
acc_test = accuracy_score(y_test, y_test_pred)
auc_test = roc_auc_score(y_test, y_test_prob, multi_class='ovr')
sens_test = recall_score(y_test, y_test_pred, average='weighted')
spec_test = precision_score(y_test, y_test_pred, average='weighted')
mcc_test = matthews_corrcoef(y_test, y_test_pred)
kappa_test = cohen_kappa_score(y_test, y_test_pred)

# Add testing metrics to results
results_df['Test Accuracy'] = acc_test
results_df['Test AUC'] = auc_test
results_df['Test Sensitivity'] = sens_test
results_df['Test Specificity'] = spec_test
results_df['Test MCC'] = mcc_test
results_df['Test Kappa'] = kappa_test

print("\nStacking Classifier Test Metrics:")
print(results_df)

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier

# Step 1: Define base classifiers
base_classifiers = [
    ('rf', RandomForestClassifier(random_state=42, n_estimators=1000, max_depth=15, min_samples_split=5, min_samples_leaf=2)),

    ('xgb', XGBClassifier(random_state=42, n_estimators=500, max_depth=10, min_child_weight=5, reg_alpha=1, reg_lambda=1)),
    ('et', ExtraTreesClassifier(random_state=42, n_estimators=500, bootstrap=True, min_samples_split=3)),
]

# Step 2: Define meta-classifier (logistic regression as a simple model)
meta_classifier = CatBoostClassifier(random_state=42, iterations=800, depth=12, learning_rate=0.2, cat_features=[], verbose=1)

# Step 3: Create a StackingClassifier
stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=meta_classifier)

# Step 4: Train and evaluate using cross-validation
results = []

for train_index, val_index in cv.split(X_train_selected, y_train):
    X_cv_train, X_cv_val = X_train_selected[train_index], X_train_selected[val_index]
    y_cv_train, y_cv_val = y_train.iloc[train_index], y_train.iloc[val_index]

    # Train the StackingClassifier
    stacking_classifier.fit(X_cv_train, y_cv_train)

    # Predictions
    y_cv_val_pred = stacking_classifier.predict(X_cv_val)
    y_cv_val_prob = stacking_classifier.predict_proba(X_cv_val)

    # Metrics
    acc_train = accuracy_score(y_cv_val, y_cv_val_pred)
    auc_train = roc_auc_score(y_cv_val, y_cv_val_prob, multi_class='ovr')
    sens_train = recall_score(y_cv_val, y_cv_val_pred, average='weighted')
    spec_train = precision_score(y_cv_val, y_cv_val_pred, average='weighted')
    mcc_train = matthews_corrcoef(y_cv_val, y_cv_val_pred)
    kappa_train = cohen_kappa_score(y_cv_val, y_cv_val_pred)

    # Store metrics
    results.append({
        'Model': 'Stacking Classifier',
        'Train Accuracy': acc_train,
        'Train AUC': auc_train,
        'Train Sensitivity': sens_train,
        'Train Specificity': spec_train,
        'Train MCC': mcc_train,
        'Train Kappa': kappa_train,
    })

# Convert results to DataFrame and display
results_df = pd.DataFrame(results)
print(results_df)

# Test the model on independent test set
stacking_classifier.fit(X_train_selected, y_train)
y_test_pred = stacking_classifier.predict(X_test_selected)
y_test_prob = stacking_classifier.predict_proba(X_test_selected)

# Test metrics
acc_test = accuracy_score(y_test, y_test_pred)
auc_test = roc_auc_score(y_test, y_test_prob, multi_class='ovr')
sens_test = recall_score(y_test, y_test_pred, average='weighted')
spec_test = precision_score(y_test, y_test_pred, average='weighted')
mcc_test = matthews_corrcoef(y_test, y_test_pred)
kappa_test = cohen_kappa_score(y_test, y_test_pred)

# Add testing metrics to results
results_df['Test Accuracy'] = acc_test
results_df['Test AUC'] = auc_test
results_df['Test Sensitivity'] = sens_test
results_df['Test Specificity'] = spec_test
results_df['Test MCC'] = mcc_test
results_df['Test Kappa'] = kappa_test

print("\nStacking Classifier Test Metrics:")
print(results_df)

